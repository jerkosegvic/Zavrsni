## Application of a Generative Language Model to Question Answering and Language Modeling Tasks
- available only in Croatian
  
This work studies the abilities of the generative language model GPT-2. The general idea behind generative language models and the principles on which they are based are explained in great detail. The architecture of GPT-2 and the mechanisms used by it are also explained. The performance of the model is measured on three different datasets in the zero-shot setup and with fine-tuning on the training subsets. The datasets test the model's ability to handle more complex language tasks. The structure of each dataset is explained in detail. For each dataset, the size of each subset is listed, and it is explained which tasks they are testing. To succeed on these tasks, the model must be able to follow the longer context and understand it well. The training procedure is described, and hyperparameters used to train each dataset are listed. A detailed analysis of the results before and after fine-tuning was carried out. The deficiencies of training procedures are commented. The steps for further development are also suggested. 
